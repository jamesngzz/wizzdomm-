import os
import base64
import json
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any

from openai import AsyncOpenAI, OpenAI

from .base_model import BaseGradingModel
from core.llm_logger import log_llm_call, SERVICE_VISION_GRADING

# Setup logging
logger = logging.getLogger(__name__)

class OpenAIModel(BaseGradingModel):
    """
    An implementation of the BaseGradingModel using OpenAI's GPT Vision models.
    """
    
    # The detailed prompt is now part of this model-specific implementation.
    VISION_GRADING_PROMPT = """
Má»™t giÃ¡o viÃªn ToÃ¡n Viá»‡t Nam tÃ i giá»i vá»›i 20 nÄƒm kinh nghiá»‡m, sá»Ÿ trÆ°á»ng cá»§a báº¡n lÃ  phÃ¢n tÃ­ch sÃ¢u sáº¯c logic giáº£i bÃ i cá»§a há»c sinh vÃ  Ä‘Æ°a ra nhá»¯ng nháº­n xÃ©t chÃ­nh xÃ¡c, cÃ´ng tÃ¢m.
**IMAGES INPUT:**
1.  **áº¢NH Äá»€ BÃ€I:** Ná»™i dung cÃ¢u há»i.
2.  **áº¢NH BÃ€I LÃ€M:** Lá»i giáº£i viáº¿t tay cá»§a há»c sinh.

### **TRIáº¾T LÃ VÃ€ QUY TRÃŒNH CHáº¤M BÃ€I**
**BÆ°á»›c 1: Äá»c Hiá»ƒu ToÃ n Diá»‡n**
Äáº§u tiÃªn, Ä‘á»c ká»¹ **áº¢NH Äá»€ BÃ€I**, náº¯m vá»¯ng yÃªu cáº§u. 
Sau Ä‘Ã³, Ä‘á»c lÆ°á»›t toÃ n bá»™ **áº¢NH BÃ€I LÃ€M** Ä‘á»ƒ hiá»ƒu luá»“ng tÆ° duy tá»•ng thá»ƒ cá»§a há»c sinh TRÆ¯á»šC KHI Ä‘i vÃ o chi tiáº¿t. 
Äá»«ng vá»™i vÃ ng phÃ¡n xÃ©t ngay tá»« lá»—i sai Ä‘áº§u tiÃªn.
**BÆ°á»›c 2: PhÃ¢n tÃ­ch Logic vÃ  TÃ¬m "Lá»—i Gá»‘c" (Root Cause Analysis)**
ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng nháº¥t. HÃ£y dÃ² theo tá»«ng bÆ°á»›c láº­p luáº­n cá»§a há»c sinh:
*   **HÆ°á»›ng Ä‘i cÃ³ Ä‘Ãºng khÃ´ng?** Há»c sinh cÃ³ chá»n Ä‘Ãºng phÆ°Æ¡ng phÃ¡p, Ä‘á»‹nh lÃ½, cÃ´ng thá»©c Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» khÃ´ng?
*   **Thá»±c thi cÃ³ chÃ­nh xÃ¡c khÃ´ng?** Trong quÃ¡ trÃ¬nh biáº¿n Ä‘á»•i, tÃ­nh toÃ¡n, há»c sinh cÃ³ máº¯c lá»—i khÃ´ng? (vÃ­ dá»¥: chuyá»ƒn váº¿ sai dáº¥u, tÃ­nh toÃ¡n sai, Ã¡p dá»¥ng sai Ä‘iá»u kiá»‡n).
*   **TÃ¬m Lá»—i Gá»‘c:** Náº¿u cÃ³ nhiá»u lá»—i sai, hÃ£y táº­p trung vÃ o **lá»—i sai Ä‘áº§u tiÃªn vÃ  cÆ¡ báº£n nháº¥t** Ä‘Ã£ gÃ¢y ra chuá»—i sai láº§m sau Ä‘Ã³. VÃ­ dá»¥, náº¿u há»c sinh tÃ­nh sai Delta ngay tá»« Ä‘áº§u, dáº«n Ä‘áº¿n toÃ n bá»™ pháº§n tÃ¬m nghiá»‡m phÃ­a sau Ä‘á»u sai, thÃ¬ "lá»—i gá»‘c" lÃ  "TÃ­nh sai biá»‡t thá»©c Delta".
*   **CÃ´ng nháº­n ná»— lá»±c:** Náº¿u há»c sinh cÃ³ hÆ°á»›ng Ä‘i Ä‘Ãºng nhÆ°ng gáº·p lá»—i tÃ­nh toÃ¡n nhá», hÃ£y ghi nháº­n pháº§n tÆ° duy Ä‘Ãºng Ä‘áº¯n Ä‘Ã³.

### **TIÃŠU CHÃ ÄÃNH GIÃ**
âœ… ÄÃšNG: Khi **phÆ°Æ¡ng phÃ¡p + Ä‘Ã¡p Ã¡n** Ä‘á»u Ä‘Ãºng. Lá»i giáº£i há»£p lÃ½ vá» máº·t toÃ¡n há»c, khÃ´ng chá»©a lá»—i logic nghiÃªm trá»ng.
ðŸ”„ ÄIá»‚M Má»˜T PHáº¦N: PhÆ°Æ¡ng phÃ¡p Ä‘Ãºng hoáº·c Ä‘Ã¡p Ã¡n Ä‘Ãºng nhÆ°ng sai sÃ³t nhá» trong tÃ­nh toÃ¡n, hoáº·c cÃ¡c lá»—i khÃ´ng Ä‘Ã¡ng ká»ƒ.
âŒ SAI: PhÆ°Æ¡ng phÃ¡p sai hoáº·c Ä‘Ã¡p Ã¡n sai hoáº·c Ä‘Ãºng má»™t cÃ¡ch "may máº¯n" nhÆ°ng cÃ³ lá»— há»•ng logic nghiá»‡m trá»ng.
âŒ KHÃ”NG LÃ€M BÃ€I: Bá» trá»‘ng hoáº·c bÃ i lÃ m khÃ´ng Ä‘á»c Ä‘Æ°á»£c.

### **YÃŠU Cáº¦U OUTPUT (Báº®T BUá»˜C)**

Báº¡n pháº£i tráº£ vá» má»™t Ä‘á»‘i tÆ°á»£ng JSON duy nháº¥t vá»›i cáº¥u trÃºc chÃ­nh xÃ¡c nhÆ° sau:

```json
{
  "is_correct": true/false,
  "confidence": float, (tá»« 0 Ä‘áº¿n 1) #Má»©c Ä‘á»™ tá»± tin cá»§a Model khi cháº¥m bÃ i
  "error_description": "Giáº£i thÃ­ch chi tiáº¿t vá» cÃ¡c lá»—i", #Náº¿u Ä‘Ãºng vÃ  khÃ´ng cÃ³ lá»—i nÃ o cáº£ thÃ¬ tráº£ vá» NULL
  "error_phrases":"Lá»—i sai há»c sinh cá»¥ thá»ƒ" (tá»‘i Ä‘a 15 tá»« má»™t Ã½, tá»‘i Ä‘a 3 Ã½) VÃ­ dá»¥: ["MÃ¢u thuáº«n logic: kháº³ng Ä‘á»‹nh (x-3)^2020+(2y+6)^2022>0 rá»“i láº¡i suy ra =0", "Äáº·t Ä‘iá»u kiá»‡n cho phÆ°Æ¡ng trÃ¬nh chá»©a cÄƒn sai, pháº£i lÃ  ... chá»© khÃ´ng lÃ  ...",...]
  "partial_credit": true/false #Trong quÃ¡ trÃ¬nh lÃ m bÃ i tá»“n táº¡i nhá»¯ng bÆ°á»›c Ä‘Ãºng (VÃ­ dá»¥ logic giáº£i bÃ i gá»“m 4 bÆ°á»›c vÃ  Ä‘Ãºng hai bÆ°á»›c Ä‘áº§u)
}
"""

    def __init__(self, api_key: str, model_name: str = "gpt-4o"):
        if not api_key:
            raise ValueError("OpenAI API key is required.")
        self.client = OpenAI(api_key=api_key)
        self.async_client = AsyncOpenAI(api_key=api_key)
        self.model_name = model_name
        logger.info(f"OpenAIModel initialized with model: {self.model_name}")

    def _encode_image(self, image_path: str) -> str:
        """Encode image to base64."""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            logger.error(f"Failed to encode image {image_path}: {e}")
            raise

    def _get_image_mime_type(self, image_path: str) -> str:
        """Determine MIME type from file extension."""
        ext = Path(image_path).suffix.lower()
        return {'.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.png': 'image/png'}.get(ext, 'image/jpeg')

    def grade_image_pair(self, question_image_paths: List[str], answer_image_paths: List[str]) -> Dict[str, Any]:
        """
        Grades a student's answer by analyzing question and answer images using OpenAI's API.
        """
        logger.info(f"Grading with OpenAI: {len(question_image_paths)} question images vs {len(answer_image_paths)} answer images.")
        logger.info(f"Question image paths: {question_image_paths}")
        logger.info(f"Answer image paths: {answer_image_paths}")
        
        try:
            message_content = [{"type": "text", "text": "HÃ£y cháº¥m bÃ i tá»± luáº­n toÃ¡n cá»§a há»c sinh."}]

            # Add question images
            for img_path in question_image_paths:
                if not os.path.exists(img_path):
                    raise FileNotFoundError(f"Question image not found: {img_path}")
                b64_image = self._encode_image(img_path)
                mime_type = self._get_image_mime_type(img_path)
                message_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:{mime_type};base64,{b64_image}", "detail": "high"}
                })

            # Add answer images
            for img_path in answer_image_paths:
                if not os.path.exists(img_path):
                    raise FileNotFoundError(f"Answer image not found: {img_path}")
                b64_image = self._encode_image(img_path)
                mime_type = self._get_image_mime_type(img_path)
                message_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:{mime_type};base64,{b64_image}", "detail": "high"}
                })

            messages = [
                {"role": "system", "content": self.VISION_GRADING_PROMPT},
                {"role": "user", "content": message_content}
            ]

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                max_completion_tokens=5000,
                response_format={"type": "json_object"}
            )

            log_llm_call(response, self.model_name, SERVICE_VISION_GRADING)
            
            result_json = json.loads(response.choices[0].message.content)
            return result_json

        except json.JSONDecodeError as e:
            logger.error(f"OpenAI response JSON parsing failed: {e}")
            raise
        except Exception as e:
            logger.error(f"OpenAI grading failed: {e}")
            raise

    async def _grade_image_pair_async(self, question_image_paths: List[str], answer_image_paths: List[str]) -> Dict[str, Any]:
        """Async version of grade_image_pair for batch processing"""
        logger.info(f"Async grading with OpenAI: {len(question_image_paths)} question images vs {len(answer_image_paths)} answer images.")
        
        try:
            message_content = [{"type": "text", "text": "HÃ£y cháº¥m bÃ i tá»± luáº­n toÃ¡n cá»§a há»c sinh."}]

            # Add question images
            for img_path in question_image_paths:
                if not os.path.exists(img_path):
                    raise FileNotFoundError(f"Question image not found: {img_path}")
                b64_image = self._encode_image(img_path)
                mime_type = self._get_image_mime_type(img_path)
                message_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:{mime_type};base64,{b64_image}", "detail": "high"}
                })

            # Add answer images
            for img_path in answer_image_paths:
                if not os.path.exists(img_path):
                    raise FileNotFoundError(f"Answer image not found: {img_path}")
                b64_image = self._encode_image(img_path)
                mime_type = self._get_image_mime_type(img_path)
                message_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:{mime_type};base64,{b64_image}", "detail": "high"}
                })

            messages = [
                {"role": "system", "content": self.VISION_GRADING_PROMPT},
                {"role": "user", "content": message_content}
            ]

            response = await self.async_client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                max_completion_tokens=5000,
                response_format={"type": "json_object"}
            )

            log_llm_call(response, self.model_name, SERVICE_VISION_GRADING)
            
            result_json = json.loads(response.choices[0].message.content)
            return result_json

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response: {e}")
            return {"is_correct": False, "confidence": 0.0, "error_description": "Failed to parse AI response", "error_phrases": [], "partial_credit": False}
        except Exception as e:
            logger.error(f"Async API request failed: {e}")
            return {"is_correct": False, "confidence": 0.0, "error_description": f"API error: {str(e)}", "error_phrases": [], "partial_credit": False}

    def grade_batch(self, items: List[Dict]) -> List[Dict[str, Any]]:
        """Override base method to use async processing with concurrency limit"""
        if not items:
            return []
            
        logger.info(f"Starting async batch grading for {len(items)} items with max 10 concurrent requests")
        return asyncio.run(self._grade_batch_async(items))
    
    async def _grade_batch_async(self, items: List[Dict]) -> List[Dict[str, Any]]:
        """Async batch processing with concurrency limit"""
        semaphore = asyncio.Semaphore(10)  # Max 10 concurrent requests
        
        async def process_item(item):
            async with semaphore:
                return await self._grade_image_pair_async(
                    question_image_paths=item['question_image_paths'],
                    answer_image_paths=item['answer_image_paths']
                )
        
        tasks = [process_item(item) for item in items]
        results = await asyncio.gather(*tasks)
        
        logger.info(f"Async batch grading completed for {len(items)} items")
        return results